{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53f2119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from env_11_26 import StockTradingEnv\n",
    "from get_data_11_26 import  get_data\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return StockTradingEnv(env_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995d20a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADS.DE\n",
      "============================================================================\n",
      "2019-11-01 00:00:00 2020-11-01 00:00:00\n",
      "(2028, 17) (252, 17)\n",
      "============================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-27 11:19:52 (running for 00:00:00.19)<br>Memory usage on this node: 1.5/13.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/7.66 GiB heap, 0.0/3.83 GiB objects<br>Result logdir: /home/zj/ray_results/PPO_2021-11-27_11-19-52<br>Number of trials: 1/5 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StockTradingEnv_f0fe1e6e</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 11:19:52,688\tINFO bayesopt.py:271 -- Skipping duplicated config: {}.\n",
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m /home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m   for external in metadata.entry_points().get(self.group, []):\n",
      "2021-11-27 11:19:53,695\tINFO bayesopt.py:271 -- Skipping duplicated config: {}.\n",
      "2021-11-27 11:19:54,697\tINFO bayesopt.py:271 -- Skipping duplicated config: {}.\n",
      "2021-11-27 11:19:55,698\tINFO bayesopt.py:271 -- Skipping duplicated config: {}.\n",
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m 2021-11-27 11:19:56,252\tINFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m 2021-11-27 11:19:56,253\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m 2021-11-27 11:19:56,253\tINFO trainer.py:772 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=72092)\u001b[0m /home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "\u001b[2m\u001b[36m(pid=72092)\u001b[0m   for external in metadata.entry_points().get(self.group, []):\n",
      "\u001b[2m\u001b[36m(pid=72095)\u001b[0m /home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "\u001b[2m\u001b[36m(pid=72095)\u001b[0m   for external in metadata.entry_points().get(self.group, []):\n",
      "\u001b[2m\u001b[36m(pid=72095)\u001b[0m 2021-11-27 11:19:59,731\tWARNING deprecation.py:39 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m 2021-11-27 11:20:00,705\tWARNING deprecation.py:39 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-27 11:20:01 (running for 00:00:09.04)<br>Memory usage on this node: 2.5/13.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/7.66 GiB heap, 0.0/3.83 GiB objects<br>Result logdir: /home/zj/ray_results/PPO_2021-11-27_11-19-52<br>Number of trials: 1/5 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StockTradingEnv_f0fe1e6e</td><td>RUNNING </td><td>10.0.0.4:72094</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m 2021-11-27 11:20:01,471\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m 2021-11-27 11:20:01,471\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-27 11:20:03 (running for 00:00:11.06)<br>Memory usage on this node: 2.5/13.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/7.66 GiB heap, 0.0/3.83 GiB objects<br>Result logdir: /home/zj/ray_results/PPO_2021-11-27_11-19-52<br>Number of trials: 1/5 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StockTradingEnv_f0fe1e6e</td><td>RUNNING </td><td>10.0.0.4:72094</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-27 11:20:08 (running for 00:00:16.07)<br>Memory usage on this node: 2.5/13.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/7.66 GiB heap, 0.0/3.83 GiB objects<br>Result logdir: /home/zj/ray_results/PPO_2021-11-27_11-19-52<br>Number of trials: 1/5 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StockTradingEnv_f0fe1e6e</td><td>RUNNING </td><td>10.0.0.4:72094</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=72094)\u001b[0m 2021-11-27 11:20:09,251\tWARNING deprecation.py:39 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "2021-11-27 11:20:11,991\tWARNING tune.py:609 -- Trial Runner checkpointing failed: Can't pickle <class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>: attribute lookup PPOTFPolicy on ray.rllib.policy.tf_policy_template failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_StockTradingEnv_f0fe1e6e:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-27_11-20-11\n",
      "  done: true\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: ea0b105024794e35997cb8a7c0cb104f\n",
      "  hostname: drl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0885748863220215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010035471990704536\n",
      "          model: {}\n",
      "          policy_loss: -0.007732681464403868\n",
      "          total_loss: 6003131904.0\n",
      "          vf_explained_var: 3.4347656310274033e-06\n",
      "          vf_loss: 6003131904.0\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.4\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.81333333333334\n",
      "    ram_util_percent: 18.113333333333333\n",
      "  pid: 72094\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 10.386147022247314\n",
      "  time_this_iter_s: 10.386147022247314\n",
      "  time_total_s: 10.386147022247314\n",
      "  timers:\n",
      "    learn_throughput: 1531.249\n",
      "    learn_time_ms: 2612.246\n",
      "    load_throughput: 13096967.994\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 514.192\n",
      "    sample_time_ms: 7779.195\n",
      "    update_time_ms: 2.137\n",
      "  timestamp: 1638012011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f0fe1e6e\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-27 11:20:11 (running for 00:00:19.54)<br>Memory usage on this node: 2.5/13.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/7.66 GiB heap, 0.0/3.83 GiB objects<br>Result logdir: /home/zj/ray_results/PPO_2021-11-27_11-19-52<br>Number of trials: 1/5 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_StockTradingEnv_f0fe1e6e</td><td>TERMINATED</td><td>10.0.0.4:72094</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.3861</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 11:20:12,103\tINFO tune.py:630 -- Total run time: 19.69 seconds (19.48 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "finish training model\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 11:20:14,267\tWARNING experiment_analysis.py:678 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
      "2021-11-27 11:20:14,270\tINFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-11-27 11:20:14,271\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2021-11-27 11:20:14,272\tINFO trainer.py:772 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m /home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m   for external in metadata.entry_points().get(self.group, []):\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m /home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m   for external in metadata.entry_points().get(self.group, []):\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m 2021-11-27 11:20:17,481\tERROR worker.py:425 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=72308, ip=10.0.0.4)\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m   File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 456, in __init__\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m   File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 1866, in <lambda>\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m     register_env(name, lambda cfg: env_object(cfg))\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m   File \"/home/zj/master_thesis/DRL_Automated_Trading/env_11_26.py\", line 11, in __init__\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m     self.data = env_config['data']\n",
      "\u001b[2m\u001b[36m(pid=72308)\u001b[0m KeyError: 'data'\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m 2021-11-27 11:20:17,571\tERROR worker.py:425 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=72309, ip=10.0.0.4)\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m   File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 456, in __init__\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m   File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 1866, in <lambda>\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m     register_env(name, lambda cfg: env_object(cfg))\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m   File \"/home/zj/master_thesis/DRL_Automated_Trading/env_11_26.py\", line 11, in __init__\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m     self.data = env_config['data']\n",
      "\u001b[2m\u001b[36m(pid=72309)\u001b[0m KeyError: 'data'\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=72308, ip=10.0.0.4)\n  File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 456, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n  File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 1866, in <lambda>\n    register_env(name, lambda cfg: env_object(cfg))\n  File \"/home/zj/master_thesis/DRL_Automated_Trading/env_11_26.py\", line 11, in __init__\n    self.data = env_config['data']\nKeyError: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_72002/1560511557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_reinit_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStockTradingEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_override_all_subkeys_if_type_changes\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0moverride_all_subkeys_if_type_changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         def _init(self, config: TrainerConfigDict,\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ray.rllib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_level\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;31m# Evaluation setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mpolicy_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 num_workers=self.config[\"num_workers\"])\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[0;34m(self, env_creator, validate_env, policy_class, config, num_workers)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mtrainer_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m             logdir=self.logdir)\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mDeveloperAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, logdir, _setup)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 remote_spaces = ray.get(self.remote_workers(\n\u001b[1;32m     88\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     lambda p, pid: (pid, p.observation_space, p.action_space)))\n\u001b[0m\u001b[1;32m     90\u001b[0m                 spaces = {\n\u001b[1;32m     91\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"original_space\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1625\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_individual_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=72308, ip=10.0.0.4)\n  File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 456, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n  File \"/home/zj/anaconda3/envs/rllib_test/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 1866, in <lambda>\n    register_env(name, lambda cfg: env_object(cfg))\n  File \"/home/zj/master_thesis/DRL_Automated_Trading/env_11_26.py\", line 11, in __init__\n    self.data = env_config['data']\nKeyError: 'data'"
     ]
    }
   ],
   "source": [
    "dax_trading_records = pd.DataFrame()\n",
    "\n",
    "for ticker in config.SYMBOLS:\n",
    "\n",
    "# for ticker in config.SYMBOLS:\n",
    "    print(ticker)\n",
    "    # get all data\n",
    "    df = get_data(ticker, config.START, config.END)\n",
    "    \n",
    "    end = datetime.strptime(config.END, \"%Y-%m-%d\")\n",
    "\n",
    "    if ticker in [\"1COV.DE\", \"DHER.DE\", \"VNA.DE\", \"ENR.DE\"]:\n",
    "        start = df.index[0]\n",
    "        training_period = start + relativedelta(years=1)\n",
    "    else:\n",
    "        start = datetime.strptime(config.START, \"%Y-%m-%d\")\n",
    "        training_period = start + relativedelta(years=8)\n",
    "\n",
    "    trading_period = training_period + relativedelta(months=config.WINDOW)\n",
    "\n",
    "    cash_balance = config.INITIAL_BALANCE\n",
    "    current_own_share = config.INITIAL_SHARE\n",
    "    \n",
    "\n",
    "    while trading_period <= end:\n",
    "\n",
    "        print(\"============================================================================\")\n",
    "        print(training_period, trading_period)\n",
    "        training_data = df[df.index < training_period]\n",
    "        trading_data = df[(df.index >= training_period) & (df.index < trading_period)]\n",
    "        print(training_data.shape, trading_data.shape)\n",
    "        print(\"============================================================================\")\n",
    "        \n",
    "        ray.init(ignore_reinit_error=True)\n",
    "        register_env(\"StockTradingEnv\", env_creator)\n",
    "\n",
    "        # rap tune.ray inside\n",
    "        analysis = tune.run(\n",
    "            run_or_experiment=PPOTrainer\n",
    "            , stop={'timesteps_total': 1e3}\n",
    "            , config={\n",
    "                'env': \"StockTradingEnv\"\n",
    "                , \"env_config\": { 'data': training_data, \"cash_balance\": config.INITIAL_BALANCE, \"current_own_share\": config.INITIAL_SHARE }\n",
    "                , \"seed\": 0\n",
    "                #             ,'lr': tune.qloguniform(1e-1, 1, 5e-5)\n",
    "                #             ,'gamma': tune.uniform(0.97, 1)\n",
    "                #             ,'clip_param' : tune.uniform(0.2, 0.4)\n",
    "                , 'vf_clip_param': 1e4\n",
    "            }\n",
    "            , search_alg=ConcurrencyLimiter(\n",
    "                BayesOptSearch(random_search_steps=4, metric=\"episode_reward_mean\", mode=\"max\"),\n",
    "                max_concurrent=2\n",
    "            )\n",
    "            , metric=\"episode_reward_mean\"\n",
    "            , mode=\"max\"\n",
    "            , num_samples=5\n",
    "            , checkpoint_at_end=True\n",
    "            , verbose=3\n",
    "        )\n",
    "        print(\"============================================================================\")\n",
    "        print(\"finish training model\")\n",
    "        print(\"============================================================================\")\n",
    "        \n",
    "        detailed_trading_results = pd.DataFrame(columns=[\"ticker\",\"date\", \"signal\", \"cash_balance\", \"share_holding\", \"asset\"\n",
    "                                                        ,\"transaction_price\", \"transaction_cost\", \"trading_outlay\", \"reward\"])\n",
    "        \n",
    "        agent = PPOTrainer(config=analysis.best_config, env=StockTradingEnv)\n",
    "        agent.restore(analysis.best_checkpoint)\n",
    "\n",
    "        # print(trading_data.head(10))\n",
    "        trade_entry = {}\n",
    "        dates = trading_data.index\n",
    "        done = False\n",
    "        env_config = {'data': trading_data, \"cash_balance\": cash_balance, \"current_own_share\": current_own_share}\n",
    "        env_trading = StockTradingEnv(env_config)\n",
    "        obs = env_trading.reset()\n",
    "\n",
    "        # first day\n",
    "        trade_entry[\"ticker\"] = ticker\n",
    "        trade_entry[\"date\"] = dates[0]\n",
    "        trade_entry[\"cash_balance\"] = trade_entry[\"asset\"] = obs[0]\n",
    "        trade_entry[\"share_holding\"] = obs[1]\n",
    "        trade_entry[\"transaction_price\"] = np.average([obs[3], obs[4]])\n",
    "        trade_entry[\"transaction_cost\"] = trade_entry[\"trading_outlay\"] = trade_entry[\"reward\"] = trade_entry[\"signal\"] = 0\n",
    "        detailed_trading_results = detailed_trading_results.append(trade_entry, ignore_index=True)\n",
    "\n",
    "        i = 1\n",
    "        while not done:\n",
    "            trade_entry[\"ticker\"] = ticker\n",
    "            trade_entry[\"date\"] = dates[i]\n",
    "            action = agent.compute_single_action(obs)\n",
    "            trade_entry[\"signal\"] = action - 1\n",
    "            obs, reward, done, info = env_trading.step(action)\n",
    "\n",
    "            trade_entry[\"cash_balance\"] = obs[0]\n",
    "            trade_entry[\"share_holding\"] = obs[1]\n",
    "            trade_entry[\"asset\"] = obs[0] + obs[1] * np.average([obs[3], obs[4]])\n",
    "            trade_entry[\"transaction_price\"] = np.average([obs[3], obs[4]])\n",
    "            trade_entry[\"transaction_cost\"] = info[\"transaction_cost\"]\n",
    "            trade_entry[\"trading_outlay\"] = info[\"trading_outlay\"]\n",
    "            trade_entry[\"reward\"] = reward\n",
    "            i += 1\n",
    "            detailed_trading_results = detailed_trading_results.append(trade_entry, ignore_index=True)\n",
    "        \n",
    "        ray.shutdown()\n",
    "        \n",
    "        cash_balance = obs[0]\n",
    "        current_own_share = obs[1]\n",
    "        print(\"============================================================================\")\n",
    "        print(cash_balance, current_own_share)\n",
    "        print(\"============================================================================\")\n",
    "\n",
    "        training_period = trading_period\n",
    "        trading_period = training_period + relativedelta(months=config.WINDOW)\n",
    "        if trading_period > end:\n",
    "            trading_period = end\n",
    "\n",
    "\n",
    "    detailed_trading_results.to_excel(saving_path + ticker + \".xlsx\")\n",
    "\n",
    "    \n",
    "#     return detailed_trading_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f45ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a95ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2b132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ray.init(ignore_reinit_error=True)\n",
    "        register_env(\"StockTradingEnv\", env_creator)\n",
    "\n",
    "        # rap tune.ray inside\n",
    "        analysis = tune.run(\n",
    "            run_or_experiment=PPOTrainer\n",
    "            , stop={'timesteps_total': 1e3}\n",
    "            , config={\n",
    "                'env': \"StockTradingEnv\"\n",
    "                , \"env_config\": { 'data': training_data, \"cash_balance\": config.INITIAL_BALANCE, \"current_own_share\": config.INITIAL_SHARE }\n",
    "                , \"seed\": 0\n",
    "                #             ,'lr': tune.qloguniform(1e-1, 1, 5e-5)\n",
    "                #             ,'gamma': tune.uniform(0.97, 1)\n",
    "                #             ,'clip_param' : tune.uniform(0.2, 0.4)\n",
    "                , 'vf_clip_param': 1e4\n",
    "            }\n",
    "            , search_alg=ConcurrencyLimiter(\n",
    "                BayesOptSearch(random_search_steps=4, metric=\"episode_reward_mean\", mode=\"max\"),\n",
    "                max_concurrent=2\n",
    "            )\n",
    "            , metric=\"episode_reward_mean\"\n",
    "            , mode=\"max\"\n",
    "            , num_samples=5\n",
    "            , checkpoint_at_end=True\n",
    "            , verbose=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3139b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a72b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc84e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rllib_test] *",
   "language": "python",
   "name": "conda-env-rllib_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
